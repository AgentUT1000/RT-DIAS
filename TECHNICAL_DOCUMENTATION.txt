================================================================================
                    RT-DIAS: COMPLETE TECHNICAL EXPLANATION
              Real-Time Disaster Information Aggregation System
================================================================================

Document Version: 1.0
Last Updated: February 2026
Author: RT-DIAS Development Team

================================================================================
                            TABLE OF CONTENTS
================================================================================

1. INTRODUCTION
2. SYSTEM OVERVIEW
3. ARCHITECTURE EXPLAINED
4. COMPONENT DETAILS
   4.1 Apache Kafka (Message Broker)
   4.2 Zookeeper (Coordination Service)
   4.3 Ingestion Service (Data Producer)
   4.4 Backend Service (Consumer + API)
   4.5 React Frontend (Dashboard)
5. DATA FLOW WALKTHROUGH
6. FILE-BY-FILE EXPLANATION
7. HOW KAFKA WORKS IN THIS PROJECT
8. WEBSOCKET COMMUNICATION
9. RUNNING THE PROJECT
10. TROUBLESHOOTING
11. FUTURE ENHANCEMENTS

================================================================================
                            1. INTRODUCTION
================================================================================

What is RT-DIAS?
----------------
RT-DIAS (Real-Time Disaster Information Aggregation System) is a real-time
disaster monitoring platform designed for India. It collects disaster-related
information from multiple sources and displays them on a unified dashboard.

Why was it built?
-----------------
During disasters like floods, cyclones, or earthquakes, information is scattered
across:
  - Twitter/X (social media posts)
  - News websites (NDTV, Times of India, etc.)
  - Government sensors (IMD weather stations)
  - Official portals (NDRF, state disaster management)

Emergency responders need ONE place to see ALL this information in REAL-TIME.
That's what RT-DIAS provides.

================================================================================
                            2. SYSTEM OVERVIEW
================================================================================

The system has 5 main parts:

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                             â”‚
â”‚   1. DATA SOURCES (Where data comes from)                                   â”‚
â”‚      â””â”€â”€ Twitter, News APIs, Government Sensors                            â”‚
â”‚                          â”‚                                                  â”‚
â”‚                          â–¼                                                  â”‚
â”‚   2. INGESTION SERVICE (Collects and sends data)                           â”‚
â”‚      â””â”€â”€ ingestion/simulateDisasters.js                                    â”‚
â”‚                          â”‚                                                  â”‚
â”‚                          â–¼                                                  â”‚
â”‚   3. APACHE KAFKA (Message broker - the heart of the system)               â”‚
â”‚      â””â”€â”€ Receives, stores, and distributes messages                        â”‚
â”‚                          â”‚                                                  â”‚
â”‚                          â–¼                                                  â”‚
â”‚   4. BACKEND SERVICE (Processes and broadcasts)                            â”‚
â”‚      â””â”€â”€ backend/server.js                                                 â”‚
â”‚                          â”‚                                                  â”‚
â”‚                          â–¼                                                  â”‚
â”‚   5. REACT DASHBOARD (User interface)                                      â”‚
â”‚      â””â”€â”€ src/components/                                                   â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

================================================================================
                        3. ARCHITECTURE EXPLAINED
================================================================================

Why This Architecture?
----------------------

Traditional Approach (Problems):
  - Frontend directly calls APIs â†’ Slow, not real-time
  - Single database â†’ Bottleneck, can't handle high traffic
  - No buffering â†’ System crashes during disasters when traffic spikes

Our Approach (Solutions):
  - Kafka buffers all messages â†’ No data loss during traffic spikes
  - WebSocket â†’ Instant updates (no refresh needed)
  - Microservices â†’ Each part can scale independently

The Message Flow:
-----------------

Step 1: Data Collection
   â””â”€â”€ Ingestion service generates/collects disaster events
   
Step 2: Send to Kafka
   â””â”€â”€ Events are published to Kafka topics
   â””â”€â”€ Each type of data goes to its own topic:
       - raw_tweets â†’ Twitter posts
       - raw_news â†’ News articles
       - sensor_data â†’ Weather/seismic data
       - processed_alerts â†’ Verified alerts

Step 3: Kafka Stores Messages
   â””â”€â”€ Messages are stored reliably
   â””â”€â”€ Multiple consumers can read the same data
   â””â”€â”€ Messages persist even if consumers are offline

Step 4: Backend Consumes
   â””â”€â”€ Backend subscribes to all topics
   â””â”€â”€ Processes each message
   â””â”€â”€ Broadcasts to connected dashboards via WebSocket

Step 5: Dashboard Displays
   â””â”€â”€ React app receives WebSocket messages
   â””â”€â”€ Updates UI in real-time
   â””â”€â”€ User sees live disaster alerts!

================================================================================
                        4. COMPONENT DETAILS
================================================================================

4.1 APACHE KAFKA (Message Broker)
---------------------------------
Location: Docker container (rtdias-kafka)
Port: 9092

What is Kafka?
  - A distributed streaming platform
  - Think of it as a "super-powered message queue"
  - Can handle millions of messages per second
  - Messages are stored reliably and can be replayed

Key Concepts:
  - TOPIC: A category for messages (like a folder)
  - PRODUCER: Sends messages to topics
  - CONSUMER: Reads messages from topics
  - BROKER: The Kafka server that stores messages

Our Topics:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Topic Name          â”‚ What it contains                              â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ raw_tweets          â”‚ Twitter-like disaster posts                   â”‚
  â”‚ raw_news            â”‚ News articles about disasters                 â”‚
  â”‚ sensor_data         â”‚ IMD weather station readings                  â”‚
  â”‚ processed_alerts    â”‚ Verified, high-priority alerts                â”‚
  â”‚ verified_incidents  â”‚ Human-verified incidents                      â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


4.2 ZOOKEEPER (Coordination Service)
-------------------------------------
Location: Docker container (rtdias-zookeeper)
Port: 2181

What is Zookeeper?
  - Manages Kafka cluster configuration
  - Tracks which brokers are alive
  - Manages leader election for partitions
  - Required for Kafka to function (in older versions)

You don't interact with Zookeeper directly - Kafka uses it internally.


4.3 INGESTION SERVICE (Data Producer)
-------------------------------------
Location: ingestion/
Files:
  - index.js           â†’ Entry point
  - kafkaClient.js     â†’ Kafka connection utilities
  - simulateDisasters.js â†’ Generates realistic disaster events

What it does:
  1. Connects to Kafka broker
  2. Creates topics if they don't exist
  3. Generates realistic disaster events every 2-5 seconds
  4. Sends events to appropriate Kafka topics

Event Generation Logic:
  - Randomly picks an Indian city (Mumbai, Chennai, Delhi, etc.)
  - Randomly picks a disaster type (flood, cyclone, earthquake, etc.)
  - Randomly assigns severity (low, medium, high, critical)
  - Creates realistic content (tweet text, news headlines, sensor readings)
  - Publishes to the appropriate topic


4.4 BACKEND SERVICE (Consumer + API)
-------------------------------------
Location: backend/
Files:
  - server.js    â†’ Main application
  - package.json â†’ Dependencies
  - Dockerfile   â†’ Docker build instructions

What it does:
  1. Starts Express.js server on port 3001
  2. Starts WebSocket server on same port
  3. Connects to Kafka as a consumer
  4. Subscribes to all disaster topics
  5. When a message arrives:
     - Parses the JSON
     - Broadcasts to all connected WebSocket clients
  6. Exposes REST API endpoints for health checks

Key Libraries:
  - kafkajs: Kafka client for Node.js
  - ws: WebSocket server
  - express: REST API server
  - cors: Cross-origin resource sharing


4.5 REACT FRONTEND (Dashboard)
------------------------------
Location: src/
Key Files:
  - hooks/useKafkaStream.js â†’ WebSocket connection hook
  - components/KafkaStatus.jsx â†’ Connection status indicator
  - components/LiveEventFeed.jsx â†’ Real-time event display
  - components/Layout.jsx â†’ Main layout
  - components/DisasterCards.jsx â†’ Disaster summary cards
  - components/DisasterDetail.jsx â†’ Detailed view

How it works:
  1. useKafkaStream hook connects to WebSocket on mount
  2. When messages arrive, they're parsed and stored in state
  3. Components re-render to show new events
  4. Statistics are updated (tweet count, news count, etc.)

Demo Mode:
  - If Kafka backend isn't running, demo mode activates
  - Generates fake events locally for testing
  - Toggle: DEMO_MODE = true/false in useKafkaStream.js

================================================================================
                        5. DATA FLOW WALKTHROUGH
================================================================================

Let's trace a single disaster event through the entire system:

STEP 1: Event Generation (ingestion/simulateDisasters.js)
----------------------------------------------------------
  - Timer triggers every 2-5 seconds
  - Random city selected: "Mumbai"
  - Random disaster type: "flood"
  - Random severity: "high"
  - Content generated: "ğŸš¨ Heavy flooding in Mumbai! Stay safe!"
  
  Event object created:
  {
    id: "tweet_1706789234_abc123",
    timestamp: "2026-01-31T10:30:00Z",
    location: { city: "Mumbai", state: "Maharashtra" },
    disaster: { type: "flood", severity: "high" },
    content: "ğŸš¨ Heavy flooding in Mumbai! Stay safe!"
  }


STEP 2: Publish to Kafka (ingestion/kafkaClient.js)
---------------------------------------------------
  - sendMessage() function called
  - Topic selected: "raw_tweets"
  - Message serialized to JSON
  - Sent to Kafka broker at kafka:29092
  
  Console output: "ğŸ“¤ [raw_tweets] Sent: tweet_1706789234_abc123"


STEP 3: Kafka Stores Message
----------------------------
  - Kafka broker receives message
  - Appends to raw_tweets topic partition
  - Assigns offset number (e.g., offset 12345)
  - Message persisted to disk
  - Consumer notified of new message


STEP 4: Backend Consumes (backend/server.js)
--------------------------------------------
  - Consumer's eachMessage() callback triggered
  - Message parsed from JSON
  - Logged: "ğŸ“¨ [raw_tweets] Received: tweet_1706789234_abc123"
  - broadcast() function called


STEP 5: WebSocket Broadcast
---------------------------
  - Loop through all connected clients
  - Send message to each client:
  {
    type: "disaster_event",
    topic: "raw_tweets",
    data: { ...the event object... }
  }


STEP 6: Frontend Receives (useKafkaStream.js)
---------------------------------------------
  - WebSocket onmessage triggered
  - Message parsed from JSON
  - processEvent() called
  - Events state updated
  - Stats incremented (tweets: 1 â†’ 2)


STEP 7: UI Updates (LiveEventFeed.jsx)
--------------------------------------
  - Component re-renders
  - New event card appears at top of list
  - Animation plays (slide-in effect)
  - User sees: "ğŸŒŠ high | Mumbai, Maharashtra | 10:30:00"


Total time from generation to display: ~50-200 milliseconds

================================================================================
                    6. FILE-BY-FILE EXPLANATION
================================================================================

DOCKER-COMPOSE.YML
------------------
Purpose: Defines all Docker containers and how they connect

Services defined:
  1. zookeeper - Coordination for Kafka
  2. kafka - Message broker
  3. kafka-ui - Web interface to monitor Kafka
  4. backend - Our Node.js backend
  5. ingestion - Our data producer

Network: All services on same Docker network (rt-dias-main_default)


BACKEND/SERVER.JS
-----------------
Purpose: Kafka consumer + WebSocket server + REST API

Key sections:
  - Lines 1-20: Import dependencies
  - Lines 22-35: Kafka configuration
  - Lines 37-50: WebSocket setup
  - Lines 52-80: Kafka consumer logic
  - Lines 82-100: REST API endpoints
  - Lines 102-120: Server startup


INGESTION/KAFKACLIENT.JS
------------------------
Purpose: Kafka producer utilities

Functions:
  - createTopics(): Creates all required topics
  - getProducer(): Returns producer singleton
  - sendMessage(): Send single message to topic
  - sendBatch(): Send multiple messages at once


INGESTION/SIMULATEDISASTERS.JS
------------------------------
Purpose: Generate realistic disaster events

Data:
  - LOCATIONS: 10 Indian cities with coordinates
  - DISASTER_TYPES: flood, cyclone, earthquake, fire, landslide, drought, heatwave
  - SEVERITIES: low, medium, high, critical

Functions:
  - generateDisasterEvent(): Creates random event
  - generateTweetContent(): Creates tweet text
  - generateNewsTitle(): Creates news headline
  - generateSensorReadings(): Creates sensor data
  - startSimulation(): Main loop that generates events


SRC/HOOKS/USEKAFKASTREAM.JS
---------------------------
Purpose: React hook for WebSocket connection

State managed:
  - isConnected: Boolean connection status
  - events: Array of received events
  - alerts: Array of high-priority alerts
  - stats: Object with counts (tweets, news, sensors, alerts)

Functions:
  - connect(): Establish WebSocket connection
  - disconnect(): Close connection
  - clearEvents(): Reset all state
  - processEvent(): Handle incoming event


SRC/COMPONENTS/KAFKASTATUS.JSX
------------------------------
Purpose: Show connection status and statistics

Displays:
  - Connection indicator (green/yellow/red dot)
  - Status text (Connected/Connecting/Disconnected)
  - Statistics grid (Tweets, News, Sensors, Alerts)


SRC/COMPONENTS/LIVEEVENTFEED.JSX
--------------------------------
Purpose: Display real-time event stream

Features:
  - Filter by disaster type
  - Filter by severity
  - Clear events button
  - Animated event cards
  - Color-coded severity badges

================================================================================
                7. HOW KAFKA WORKS IN THIS PROJECT
================================================================================

Why Kafka Instead of Direct API Calls?
--------------------------------------

Without Kafka:
  [Data Source] â†’ [Backend] â†’ [Database] â†’ [Frontend]
  
  Problems:
  - Backend can be overwhelmed during disasters
  - If backend crashes, data is lost
  - Can't replay past events
  - All consumers get same processing load

With Kafka:
  [Data Source] â†’ [Kafka] â†’ [Backend] â†’ [Frontend]
  
  Benefits:
  - Kafka buffers messages (handles traffic spikes)
  - Messages persist even if backend is down
  - Multiple consumers can read independently
  - Can replay events for analysis


Kafka Topics in Detail
----------------------

1. raw_tweets
   - Source: Twitter/X API (simulated)
   - Content: Social media posts about disasters
   - Volume: High (many tweets during disaster)
   - Use: Crowd-sourced information

2. raw_news
   - Source: GDELT, news scrapers
   - Content: News articles, headlines
   - Volume: Medium
   - Use: Verified news from media

3. sensor_data
   - Source: IMD, weather stations
   - Content: Temperature, water levels, seismic readings
   - Volume: Low-Medium (periodic readings)
   - Use: Official government data

4. processed_alerts
   - Source: AI/ML processing pipeline
   - Content: Verified, classified alerts
   - Volume: Low (filtered, important only)
   - Use: High-priority notifications


Consumer Groups
---------------
Our backend uses consumer group: "rtdias-dashboard-group"

This means:
  - Multiple backend instances share the work
  - Each message is processed by ONE instance
  - If one backend crashes, others take over
  - Kafka tracks what's been consumed

================================================================================
                    8. WEBSOCKET COMMUNICATION
================================================================================

Why WebSocket Instead of HTTP Polling?
--------------------------------------

HTTP Polling:
  - Frontend asks "Any new data?" every second
  - Server responds "No" or sends data
  - Wasteful: many empty requests
  - Latency: up to 1 second delay

WebSocket:
  - Single persistent connection
  - Server pushes data instantly when available
  - Efficient: no empty requests
  - Real-time: millisecond latency


Message Format
--------------

Connection message (sent on connect):
{
  type: "connection",
  message: "Connected to RT-DIAS Kafka Stream",
  topics: ["raw_tweets", "raw_news", "sensor_data", "processed_alerts"],
  timestamp: "2026-01-31T10:30:00Z"
}

Event message (sent on each Kafka message):
{
  type: "disaster_event",
  topic: "raw_tweets",
  partition: 0,
  offset: "12345",
  timestamp: "1706789234000",
  data: {
    id: "tweet_1706789234_abc123",
    location: { city: "Mumbai", state: "Maharashtra" },
    disaster: { type: "flood", severity: "high" },
    content: "ğŸš¨ Heavy flooding in Mumbai!"
  }
}


Client Handling (useKafkaStream.js)
-----------------------------------

1. On mount: connect()
2. On open: Set isConnected = true
3. On message: Parse JSON, call processEvent()
4. On close: Attempt reconnection after 3 seconds
5. On error: Set status to "error"
6. On unmount: disconnect()

================================================================================
                        9. RUNNING THE PROJECT
================================================================================

Prerequisites
-------------
1. Docker Desktop - https://www.docker.com/products/docker-desktop/
2. Node.js 20+ - https://nodejs.org/
3. Git - https://git-scm.com/

Step-by-Step Instructions
-------------------------

STEP 1: Clone Repository
  git clone https://github.com/AgentUT1000/RT-DIAS.git
  cd RT-DIAS
  git checkout Test-branch-1

STEP 2: Start Docker Containers
  docker compose up -d
  
  This starts:
  - Zookeeper (port 2181)
  - Kafka (port 9092)
  - Kafka UI (port 8080)
  - Backend (port 3001)
  - Ingestion (internal)

STEP 3: Wait for Services
  docker compose ps
  
  All should show "Up" status. Wait 30-60 seconds for Kafka to fully start.

STEP 4: Start Frontend
  npm install
  npm run dev

STEP 5: Open Browser
  - Dashboard: http://localhost:5173
  - Kafka UI: http://localhost:8080

STEP 6: Test
  - Click "ğŸ“¡ Live Kafka Feed" button
  - You should see events appearing every few seconds
  - Check Kafka UI to see messages in topics


Stopping Everything
-------------------
  # Stop frontend: Ctrl+C in terminal
  
  # Stop Docker containers:
  docker compose down
  
  # Remove all data:
  docker compose down -v

================================================================================
                        10. TROUBLESHOOTING
================================================================================

Problem: "Kafka UI shows no messages"
-------------------------------------
Cause: Ingestion service may have crashed
Solution: 
  docker logs rtdias-ingestion
  docker restart rtdias-ingestion


Problem: "Dashboard shows 'Disconnected'"
-----------------------------------------
Cause: Backend not running or WebSocket blocked
Solution:
  1. Check backend: docker logs rtdias-backend
  2. Verify port 3001 is accessible
  3. Check browser console for errors


Problem: "docker compose up fails"
----------------------------------
Cause: Docker not running or ports in use
Solution:
  1. Open Docker Desktop
  2. Wait for it to fully start
  3. Check if ports 2181, 3001, 8080, 9092 are free


Problem: "Events not appearing"
-------------------------------
Cause: Kafka topics may not be created
Solution:
  1. Open Kafka UI: http://localhost:8080
  2. Check if topics exist
  3. Restart ingestion: docker restart rtdias-ingestion


Problem: "Cannot connect to Kafka"
----------------------------------
Cause: Kafka not fully started
Solution:
  Wait 60 seconds after docker compose up
  Check: docker logs rtdias-kafka

================================================================================
                    11. FUTURE ENHANCEMENTS
================================================================================

Planned Features
----------------
1. Real Twitter/X API integration
2. GDELT news API connection
3. IMD sensor API integration
4. Google Earth Engine satellite imagery
5. AI/NLP for disaster classification
6. PostgreSQL + PostGIS for storage
7. Interactive map with disaster markers
8. Push notifications
9. Historical data analysis
10. Multi-language support (Hindi, regional languages)


Scaling Considerations
----------------------
- Add more Kafka partitions for higher throughput
- Deploy multiple backend instances
- Use Kubernetes for container orchestration
- Add Redis for caching
- Implement Apache Flink for stream processing


Security Enhancements
---------------------
- Add authentication (JWT)
- Enable Kafka SSL/TLS
- Implement rate limiting
- Add input validation
- Set up audit logging

================================================================================
                            END OF DOCUMENT
================================================================================

For questions or issues, please open a GitHub issue at:
https://github.com/AgentUT1000/RT-DIAS/issues

================================================================================
